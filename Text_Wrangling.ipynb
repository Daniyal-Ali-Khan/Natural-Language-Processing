{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "M76EubdoqXCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnG3aoWLqZgU",
        "outputId": "f82a99f4-0173-40a6-9926-d233aca06940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case Conversion**"
      ],
      "metadata": {
        "id": "IJeMnFmTr6Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The quick brown fox jumped over The Big Dog'\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Iu-lu9_9r_JW",
        "outputId": "2642cf57-6f3a-46c2-ce84-92d249a0af44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The quick brown fox jumped over The Big Dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n2C1FI5tsCKa",
        "outputId": "f91481eb-0d77-4a38-a2b0-a3600bd4aee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the quick brown fox jumped over the big dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.upper()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j3kRHe4NsFFE",
        "outputId": "d6a195c4-1e9f-4e8a-eab7-5caaca842db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'THE QUICK BROWN FOX JUMPED OVER THE BIG DOG'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.title()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PnLQNnh6sIKj",
        "outputId": "0a2e54e2-0475-48d4-cb0c-b3a7e593414f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Quick Brown Fox Jumped Over The Big Dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "rCL6vebFsdAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \"\n",
        "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \"\n",
        "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "               \"which reportedly take up the size of two tennis courts.\")\n",
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Cp9uVYPDsf1h",
        "outputId": "c3a1e07b-6cf0-405d-8b19-c50e12b87365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In this code it seperated the lines where full stop (.) ends\n",
        "import nltk\n",
        "\n",
        "nltk.sent_tokenize(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XubL329cskDY",
        "outputId": "115be7d9-fc45-4894-de10-1b6818545904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
              " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
              " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
              " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#It seperates every word\n",
        "print(nltk.word_tokenize(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh7BnE3CtMHJ",
        "outputId": "87cccf6b-7d4d-405b-f325-398a5fe92c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1-This line imports the spaCy library, which is a powerful and efficient NLP library in Python.\n",
        "import spacy\n",
        "#2-Load English Model:\n",
        "#This line loads the English language model \"en_core_web_sm\" provided by spaCy. The model includes pre-trained statistical models and word vectors for\n",
        "#the English language. \"sm\" stands for \"small,\" indicating that it's a smaller model suitable for most general-purpose NLP tasks.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#3-Process Text:\n",
        "#This line processes the sample_text using the loaded spaCy model. It tokenizes the text, performs part-of-speech tagging, entity recognition,\n",
        "#and other linguistic annotations. The result is stored in the text_spacy object, which is a spaCy Doc object.\n",
        "text_spacy = nlp(sample_text)"
      ],
      "metadata": {
        "id": "dTmhgBWUtXo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This line of code extracts the individual sentences from the spaCy processed text (text_spacy). Let's break down how this code achieves that\n",
        "[obj.text for obj in text_spacy.sents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXNeVPRVti7X",
        "outputId": "6890af61-7aaa-4555-ae9d-73721c06ad25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
              " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
              " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
              " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([obj.text for obj in text_spacy])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUf_sBCPvOex",
        "outputId": "a12abc8e-400f-4f21-912d-352d241caf35"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing HTML tags & noise**"
      ],
      "metadata": {
        "id": "WYNa9XxewxVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.text\n",
        "print(content[2745:3948])"
      ],
      "metadata": {
        "id": "3bGFbPyBwy74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b637c1-be33-476a-fd27-e62ec09ca349"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ne spacing (\"leading\") */\r\n",
            "    }\r\n",
            "body > p {\r\n",
            "    /* paras at <body> level - not in <div> or <table>  */\r\n",
            "    text-align: justify;\r\n",
            "    /* or left?? */\r\n",
            "    text-indent: 1em;\r\n",
            "    /* first-line indent */\r\n",
            "    }\r\n",
            "/* suppress indentation on paragraphs following heads  */\r\n",
            "h2 + p, h3 + p, h4 + p {\r\n",
            "    text-indent: 0\r\n",
            "    }\r\n",
            "/* tighter spacing for list item paragraphs */\r\n",
            "dd, li {\r\n",
            "    margin-top: 0.25em;\r\n",
            "    margin-bottom: 0;\r\n",
            "    line-height: 1.2em;\r\n",
            "    /* a bit closer than p's */\r\n",
            "    }\r\n",
            "/* ************************************************************************\r\n",
            " * Head 2 is for chapter heads. \r\n",
            " * ********************************************************************** */\r\n",
            "h2 {\r\n",
            "    /* text-align:center;  left-aligned by default. */\r\n",
            "    margin-top: 3em;\r\n",
            "    /* extra space above.. */\r\n",
            "    margin-bottom: 2em;\r\n",
            "    /* ..and below */\r\n",
            "    clear: both;\r\n",
            "    /* don't let sidebars overlap */\r\n",
            "    }\r\n",
            "/* ************************************************************************\r\n",
            " * Head 3 is for main-topic heads.\r\n",
            " * ********************************************************************** */\r\n",
            "h3 {\r\n",
            "    /* text-align:center;  left-aligned by default. */\r\n",
            "    margin-top: 2em;\r\n",
            "    /* extra space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "clean_content = strip_html_tags(content)\n",
        "print(clean_content[1163:1957])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngblt2CgNNf6",
        "outputId": "8459082d-3a69-4ec1-f7e0-0d96b04b861e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "form, and void; and darkness was\n",
            "           upon the face of the deep. And the Spirit of God moved upon\n",
            "           the face of the waters.\n",
            "01:001:003 And God said, Let there be light: and there was light.\n",
            "01:001:004 And God saw the light, that it was good: and God divided the\n",
            "           light from the darkness.\n",
            "01:001:005 And God called the light Day, and the darkness he called\n",
            "           Night. And the evening and the morning were the first day.\n",
            "01:001:006 And God said, Let there be a firmament in the midst of the\n",
            "           waters, and let it divide the waters from the waters.\n",
            "01:001:007 And God made the firmament, and divided the waters which were\n",
            "           under the firmament from the waters which were above the\n",
            "           firmament: and it was so.\n",
            "01:001:008 And God called the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Accented Characters**"
      ],
      "metadata": {
        "id": "2keVp561Ns43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ],
      "metadata": {
        "id": "YjVf3eBENvCY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'Sómě Áccěntěd těxt'\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hzy6iq-bNzIS",
        "outputId": "3b1e8deb-a56e-45a8-e507-91606297965a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sómě Áccěntěd těxt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_accented_chars(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-xx_ptUAN3pi",
        "outputId": "be11f9d4-213e-42f4-9727-bf7be750fe4d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Special Characters, Numbers and Symbols**"
      ],
      "metadata": {
        "id": "qXTxpcSzOACm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "txAT1g1mOBtX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LcdDZbhAOFbu",
        "outputId": "80fa4b99-c845-4ba9-f0e6-62333ef4173e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_special_characters(s, remove_digits=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SxOMWGkzOIjy",
        "outputId": "2a2daaeb-f644-49ea-cef6-8dee3067e5b3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun See you at  What do you think  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_special_characters(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2blqnWoXOMOr",
        "outputId": "16ffb3a9-6ffc-4de5-b11a-ef17cce6cf13"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun See you at 730 What do you think 9318 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expanding Contractions**"
      ],
      "metadata": {
        "id": "RbWJzJkvOU-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS8HE_uHOWoz",
        "outputId": "b4eb9018-41d1-4f8f-ec89-5217c81d99e8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.10/dist-packages (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "APlrUgJAOamH",
        "outputId": "7061ee6f-9b5c-498b-f6a1-8ef84fd4697a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "list(contractions.contractions_dict.items())[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIYPctB2OgmQ",
        "outputId": "d0549f8c-5206-4c2b-9b92-b6550f640efd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"I'm\", 'I am'),\n",
              " (\"I'm'a\", 'I am about to'),\n",
              " (\"I'm'o\", 'I am going to'),\n",
              " (\"I've\", 'I have'),\n",
              " (\"I'll\", 'I will'),\n",
              " (\"I'll've\", 'I will have'),\n",
              " (\"I'd\", 'I would'),\n",
              " (\"I'd've\", 'I would have'),\n",
              " ('Whatcha', 'What are you'),\n",
              " (\"amn't\", 'am not')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contractions.fix(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H7dGcM9lPqO-",
        "outputId": "b93aa41f-b020-4966-c411-35bfbb4378d6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You all cannot expand contractions I would think! You would not be able to. How did you do it?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "tyqLxgMkPzc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYpCbfwCP1IE",
        "outputId": "39ccfcf6-f81a-49a5-b634-0b3dca485d96"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('lying')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6OEFlN9hP47L",
        "outputId": "34e02d28-f486-43f0-c690-40312e131874"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('strange')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lh2M9nNDP-LJ",
        "outputId": "af0aeab3-3ff2-4f27-a94a-40238d550adb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'strang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "Z4J_397DQD9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "qCk_rKNrQFfO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(wnl.lemmatize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjAHQw7wQKef",
        "outputId": "d65e96f7-078c-457c-d3d3-08f5000e8989"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method lemmatize in module nltk.stem.wordnet:\n",
            "\n",
            "lemmatize(word: str, pos: str = 'n') -> str method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
            "    Lemmatize `word` using WordNet's built-in morphy function.\n",
            "    Returns the input word unchanged if it cannot be found in WordNet.\n",
            "    \n",
            "    :param word: The input word to lemmatize.\n",
            "    :type word: str\n",
            "    :param pos: The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
            "        `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
            "        for satellite adjectives.\n",
            "    :param pos: str\n",
            "    :return: The lemma of `word`, for the given `pos`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatize nouns\n",
        "print(wnl.lemmatize('cars', 'n'))\n",
        "print(wnl.lemmatize('boxes', 'n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eshVSLnQN74",
        "outputId": "82176424-77bd-47dc-bcbd-a340296573bc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car\n",
            "box\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatize verbs\n",
        "print(wnl.lemmatize('running', 'v'))\n",
        "print(wnl.lemmatize('ate', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM12MwE9QS_t",
        "outputId": "c4d68aff-0489-4bf0-c715-e16997c7e6ef"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatize adjectives\n",
        "print(wnl.lemmatize('saddest', 'a'))\n",
        "print(wnl.lemmatize('fancier', 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsC5zgseQXIu",
        "outputId": "403a0051-87d4-44dc-f7ba-a93a4e3bbfd6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sad\n",
            "fancy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ineffective lemmatization\n",
        "print(wnl.lemmatize('ate', 'n'))\n",
        "print(wnl.lemmatize('fancier', 'v'))\n",
        "print(wnl.lemmatize('fancier'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CluNmambQaTv",
        "outputId": "f06c663a-5484-4211-e38f-a0309d31d30b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ate\n",
            "fancier\n",
            "fancier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ],
      "metadata": {
        "id": "bu8wUI6uQcum"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize**"
      ],
      "metadata": {
        "id": "96hYAfvzQjGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(s)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS4mmrr4Qkve",
        "outputId": "12fd18ef-1251-4fcb-d8f2-f3ef97f1c830"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_text = ' '.join(wnl.lemmatize(token) for token in tokens)\n",
        "lemmatized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BJ_kglWiQn8u",
        "outputId": "06741b90-411f-453b-b453-13479ec45964"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox are quick and they are jumping over the sleeping lazy dog !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS Tagging**"
      ],
      "metadata": {
        "id": "xEchh5ECQuTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "print(tagged_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA_vtbi1Qwi2",
        "outputId": "a30f1036-dcae-4b98-e44a-0633a0ad3a2e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('brown', 'JJ'), ('foxes', 'NNS'), ('are', 'VBP'), ('quick', 'JJ'), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('sleeping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tag conversion to WordNet Tags**"
      ],
      "metadata": {
        "id": "SDmtVd6TSOu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def pos_tag_wordnet(tagged_tokens):\n",
        "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
        "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
        "                            for word, tag in tagged_tokens]\n",
        "    return new_tagged_tokens"
      ],
      "metadata": {
        "id": "cBiBcgfzSR4B"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
        "print(wordnet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtUgapCPSVs-",
        "outputId": "ebb7b56e-6d7b-4085-b535-5f83d60cba6e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'n'), ('brown', 'a'), ('foxes', 'n'), ('are', 'v'), ('quick', 'a'), ('and', 'n'), ('they', 'n'), ('are', 'v'), ('jumping', 'v'), ('over', 'n'), ('the', 'n'), ('sleeping', 'v'), ('lazy', 'a'), ('dogs', 'n'), ('!', 'n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Effective Lemmatization**\n"
      ],
      "metadata": {
        "id": "bAXxgh5kTDgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
        "lemmatized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p6eWiuMsTLLF",
        "outputId": "ec969a88-8e46-4352-bf7a-bc603306a4e8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox be quick and they be jump over the sleep lazy dog !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ZhqkTQLTPQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn: Define a function such that you put all the above steps together so that it does the following**\n",
        "\n",
        "*Function name is wordnet_lemmatize_text(...)\n",
        "\n",
        "Input is a variable text which should take in a document (bunch of words)\n",
        "\n",
        "Call the earlier defined functions and utilize them\n",
        "\n",
        "Return lemmatized text as the output (as a string)*"
      ],
      "metadata": {
        "id": "5mm_4XjKTZjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def wordnet_lemmatize_text(text):\n",
        "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
        "    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
        "    return lemmatized_text"
      ],
      "metadata": {
        "id": "19JgLD1rTbo0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Turn: Now call the function on the below sentence and test it**"
      ],
      "metadata": {
        "id": "peTBGKd5TvCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y0-c-_SLTwu_",
        "outputId": "9fad6f14-d331-4cbd-fa29-8c20403e2151"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatize_text(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GGwuVhkXT1jg",
        "outputId": "1efad051-92d8-49f6-eb2a-26e307277c46"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox be quick and they be jump over the sleep lazy dog !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization with Spacy**"
      ],
      "metadata": {
        "id": "V1YQvt9wT7Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model without disabling components\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Disable parsing, tagging, and entity recognition\n",
        "nlp.disable_pipe(\"parser\")\n",
        "nlp.disable_pipe(\"tagger\")\n",
        "nlp.disable_pipe(\"ner\")\n",
        "\n",
        "def spacy_lemmatize_text(text):\n",
        "    # Process the text using the modified spaCy model\n",
        "    text = nlp(text)\n",
        "\n",
        "    # Lemmatize the text\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "4_G4IY_EUAsj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b1wfyAdbUo7-",
        "outputId": "83a86278-52d5-4c46-de02-a145508f6765"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_lemmatize_text(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Y_edTZuiUvKl",
        "outputId": "4c1122df-3b75-4e50-dea3-6791bbf48571"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the brown foxes are quick and they are jumping over the sleeping lazy dogs !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopword Removal**"
      ],
      "metadata": {
        "id": "3kXYGqd9U4Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text"
      ],
      "metadata": {
        "id": "OGkumDmMUyFH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "print(stop_words[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJCalW2qU9lk",
        "outputId": "ad81a2c5-f226-45ca-ee53-b4997dbf2adb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZkreyazWVAqo",
        "outputId": "e21bdd38-768d-4999-97f0-4bafd955f34e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn: Remove the words 'the' and 'brown' from the stop_words list and call the function with this new list**"
      ],
      "metadata": {
        "id": "wJiZZvhLVQvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words.remove('the')\n",
        "stop_words.append('brown')"
      ],
      "metadata": {
        "id": "hhye07odVS9o"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_stopwords(s, is_lower_case=False, stopwords=stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YslJDZDPVWbX",
        "outputId": "7a055968-a680-47e4-faf0-1dffe88f4a3e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The foxes quick jumping the sleeping lazy dogs !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzXyANNIVYlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}